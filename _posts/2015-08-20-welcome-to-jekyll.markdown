---
layout: post
title:  "August 20, 2015"
date:   2015-08-20 22:35:59
categories: jekyll update
---
####The purpose of this blog is to share the progress of a research project pertaining to machine-learning and music improvisation developed by data scientist Juan Hernandez and composer Scott Rubin. Both researchers are currently based in Berkeley, California. The funding for this research was generously provided by the University of California, Berkeley Digital Humanities Collaborative Research Grant, with support from the Andrew W. Mellon Foundation. We are also thankful for technological support from the Center for New Music and Audio Technologies (CNMAT).

###**Here’s a bit of background on our project:**

####We will create a software program that interacts with human musicians to automatically co-author music in real time using machine learning. Our real-time interactive system will contribute to and draw from already existing branches of study in music composition and computer science. From computer science, the system will apply techniques from Music Information Retrieval (MIR) and Machine Learning to analyze and generate musical content. Within the domain of music composition, our piece aims to develop an interactive digital framework for gesture-based music improvisation. This latter point distinguishes the project from previous work in machine improvisation/composition which, for the most part, has historically attempted to mimic musical styles by generating melodies. In contemporary practices of music improvisation and performance, melody is not the sole descriptor of a musical style. Performers communicate more with musical actions and sonic events within a conditioned space, rather than pre-established harmonic movement or melodic movement.

The distinction between melodic and gesture-based or topological music underlies what we believe to be one of the major innovations of this project. Much of the previous work in machine improvisation begins with a symbolic representation of music that reduces sounds to pitch and duration (i.e. melody). This reduction results in machine improvisors with limited capabilities: they can produce sequences of notes, but they are unable to work with timbral and textural or other higher order qualities of music because they are excluded from the symbolic representation.

Our interactive system will be used in the creation of a new musical work which Scott will compose. This work will be scored for three improvising musicians (tentative instrumentation: piano, clarinet, and cello) and interactive digital system. Scott will use a flexible score will serve to guide the musicians in the creation of improvised textures, interactive situations, and a malleable large-scale form that may be stretched and compressed at the musicians’ will. The use of a score will allow us to have a high level control of the performance and will also allow us to create labels for training data. This paradigm provides an opportunity to create idiomatic, novel, and complex textures that do not rely on explicit notation or fixed instrumental technique. Rather, each musician will find their own techniques of interpreting the notation depending on what works most idiomatically on their instrument. 

From a technological perspective, the project employs tools from Music Information Retrieval and Machine Learning. More specifically we will use a library of feature extraction techniques to process live data to form the digital representation of the acoustic signal. Through digital representation, we can use a variety of machine learning techniques for different purposes. We can use hierarchical and non-hierarchical methods for song segmentation and sound clustering, which allows us to represent higher-order features that describe the structure of the piece. We can use gesture recognition techniques and other pattern recognition techniques and apply them to music to identify specific musical gestures or musical phrases that are performed by the musicians. In effect, using machine learning will create a semantic representation of the music, greater than the mere sum of the acoustic signals.

Within the framework of this piece, our interactive system will listen to the improvisers’ sound and respond in a way “learned” from previous musical situations. This learning process will consist of feeding the system data from recorded improvisations, and allowing the machine to interpret this data. Through this process, the system will learn how to recognize musical context and gestures and contribute to the improvisation using behavior-conditioned prior training data. In this way, the system will function as another improvisor, adding to the musical discourse and potentially altering the musical landscape of the piece.
_____________________

Don’t worry, we’re overwhelmed too. Through the course of this project, we will do our very best to explain our research clearly for the general public, as well as share our thoughts and philosophies regarding music improvisation, the role of the computer in music performance, music notation, machine learning, music information retrieval, programming in Python and Max/MSP, and a variety of other topics that we will encounter through the course of this project. 

###_Cheers!_
